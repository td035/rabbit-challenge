{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実装演習 4-1. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common import functions\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ用意\n",
    "2進数の予測値を実データに近づけていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2進数の桁数\n",
    "binary_dim = 8\n",
    "# 最大値 + 1\n",
    "largest_number = pow(2, binary_dim)\n",
    "# largest_numberまで2進数を用意\n",
    "binary = np.unpackbits(np.array([range(largest_number)], dtype=np.uint8).T, axis=1)\n",
    "\n",
    "input_layer_size = 2  # 入力層の数\n",
    "hidden_layer_size = 16  # 隠れ層の数\n",
    "output_layer_size = 1  # 出力層の数\n",
    "\n",
    "weight_init_std = 1  # 初期ウェイト\n",
    "learning_rate = 0.1  # 学習率\n",
    "\n",
    "iters_num = 10000  # 繰り返し回数\n",
    "plot_interval = 100  # グラフへのプロット間隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ウェイト初期化 (バイアスは簡単のため省略)\n",
    "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
    "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
    "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配\n",
    "W_in_grad = np.zeros_like(W_in)\n",
    "W_out_grad = np.zeros_like(W_out)\n",
    "W_grad = np.zeros_like(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
    "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
    "y = np.zeros((output_layer_size, binary_dim))\n",
    "\n",
    "delta_out = np.zeros((output_layer_size, binary_dim))\n",
    "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
    "\n",
    "all_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters:0\n",
      "Loss:1.469399340903874\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 0 0 1 0 0]\n",
      "125 + 39 = 0\n",
      "------------\n",
      "iters:100\n",
      "Loss:1.3968157974462927\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 1 1 1 0 1]\n",
      "73 + 20 = 0\n",
      "------------\n",
      "iters:200\n",
      "Loss:0.9470550270404142\n",
      "Pred:[0 0 1 0 1 1 1 0]\n",
      "True:[1 0 0 0 0 0 1 0]\n",
      "20 + 110 = 46\n",
      "------------\n",
      "iters:300\n",
      "Loss:0.898586917981236\n",
      "Pred:[0 0 1 1 1 1 1 0]\n",
      "True:[0 0 1 0 1 0 1 0]\n",
      "12 + 30 = 62\n",
      "------------\n",
      "iters:400\n",
      "Loss:0.8959399431187767\n",
      "Pred:[1 1 1 0 0 1 1 1]\n",
      "True:[1 1 0 0 0 1 1 0]\n",
      "99 + 99 = 231\n",
      "------------\n",
      "iters:500\n",
      "Loss:1.0006066872760597\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 0 0 0 1 0]\n",
      "121 + 41 = 0\n",
      "------------\n",
      "iters:600\n",
      "Loss:1.2948714529648264\n",
      "Pred:[1 1 1 1 1 1 1 1]\n",
      "True:[0 1 1 1 0 0 0 0]\n",
      "25 + 87 = 255\n",
      "------------\n",
      "iters:700\n",
      "Loss:0.7563806366283264\n",
      "Pred:[0 0 0 1 0 0 0 0]\n",
      "True:[0 1 0 1 1 0 0 0]\n",
      "8 + 80 = 16\n",
      "------------\n",
      "iters:800\n",
      "Loss:0.9094656360238291\n",
      "Pred:[0 0 1 0 0 0 0 1]\n",
      "True:[1 0 1 0 1 0 1 1]\n",
      "90 + 81 = 33\n",
      "------------\n",
      "iters:900\n",
      "Loss:0.7902618074464609\n",
      "Pred:[1 0 0 0 0 0 0 0]\n",
      "True:[1 1 0 1 0 1 0 0]\n",
      "112 + 100 = 128\n",
      "------------\n",
      "iters:1000\n",
      "Loss:0.9211398305743703\n",
      "Pred:[1 0 1 0 1 1 0 1]\n",
      "True:[1 0 1 1 0 0 0 1]\n",
      "91 + 86 = 173\n",
      "------------\n",
      "iters:1100\n",
      "Loss:0.7954160074390352\n",
      "Pred:[1 0 0 0 1 0 0 1]\n",
      "True:[1 0 0 1 1 0 0 1]\n",
      "69 + 84 = 137\n",
      "------------\n",
      "iters:1200\n",
      "Loss:0.7732222669644866\n",
      "Pred:[0 1 0 1 0 1 0 0]\n",
      "True:[0 1 1 0 0 1 0 0]\n",
      "74 + 26 = 84\n",
      "------------\n",
      "iters:1300\n",
      "Loss:1.2674862358936478\n",
      "Pred:[0 1 1 1 1 1 0 1]\n",
      "True:[1 0 0 0 0 1 1 0]\n",
      "63 + 71 = 125\n",
      "------------\n",
      "iters:1400\n",
      "Loss:0.7383277142252909\n",
      "Pred:[0 1 1 0 0 1 0 0]\n",
      "True:[0 1 1 1 0 1 1 0]\n",
      "26 + 92 = 100\n",
      "------------\n",
      "iters:1500\n",
      "Loss:0.9633763713806666\n",
      "Pred:[1 1 1 0 0 0 0 1]\n",
      "True:[1 0 0 1 0 0 1 1]\n",
      "88 + 59 = 225\n",
      "------------\n",
      "iters:1600\n",
      "Loss:0.5343469302392587\n",
      "Pred:[1 0 1 0 0 1 0 1]\n",
      "True:[1 0 1 0 0 1 0 1]\n",
      "82 + 83 = 165\n",
      "------------\n",
      "iters:1700\n",
      "Loss:1.0577993876605503\n",
      "Pred:[0 1 1 1 0 1 0 0]\n",
      "True:[0 1 1 0 1 0 1 0]\n",
      "75 + 31 = 116\n",
      "------------\n",
      "iters:1800\n",
      "Loss:1.049026115162555\n",
      "Pred:[0 1 1 0 1 1 0 1]\n",
      "True:[1 0 0 0 1 1 0 1]\n",
      "18 + 123 = 109\n",
      "------------\n",
      "iters:1900\n",
      "Loss:1.0180176551915132\n",
      "Pred:[0 1 1 0 1 1 1 0]\n",
      "True:[0 1 1 1 0 0 0 0]\n",
      "25 + 87 = 110\n",
      "------------\n",
      "iters:2000\n",
      "Loss:0.8358687420988284\n",
      "Pred:[1 0 0 1 0 0 0 1]\n",
      "True:[1 1 1 0 0 0 0 1]\n",
      "100 + 125 = 145\n",
      "------------\n",
      "iters:2100\n",
      "Loss:0.6067498381941865\n",
      "Pred:[0 1 1 0 0 1 1 1]\n",
      "True:[0 1 1 0 0 1 1 1]\n",
      "41 + 62 = 103\n",
      "------------\n",
      "iters:2200\n",
      "Loss:0.42347048484263594\n",
      "Pred:[0 0 1 0 1 0 0 1]\n",
      "True:[0 0 1 0 1 0 0 1]\n",
      "38 + 3 = 41\n",
      "------------\n",
      "iters:2300\n",
      "Loss:0.5082489142647703\n",
      "Pred:[0 0 1 0 1 1 0 1]\n",
      "True:[0 0 1 1 1 1 0 1]\n",
      "38 + 23 = 45\n",
      "------------\n",
      "iters:2400\n",
      "Loss:0.16071637203601652\n",
      "Pred:[0 1 0 0 0 0 1 0]\n",
      "True:[0 1 0 0 0 0 1 0]\n",
      "2 + 64 = 66\n",
      "------------\n",
      "iters:2500\n",
      "Loss:0.531136561694967\n",
      "Pred:[0 1 1 1 1 0 1 1]\n",
      "True:[0 0 1 1 1 0 1 1]\n",
      "59 + 0 = 123\n",
      "------------\n",
      "iters:2600\n",
      "Loss:0.7736744678310178\n",
      "Pred:[0 1 1 0 0 1 1 1]\n",
      "True:[0 1 0 1 1 1 1 1]\n",
      "18 + 77 = 103\n",
      "------------\n",
      "iters:2700\n",
      "Loss:0.35484779080075274\n",
      "Pred:[0 1 0 1 0 1 1 1]\n",
      "True:[0 1 0 1 0 1 1 1]\n",
      "31 + 56 = 87\n",
      "------------\n",
      "iters:2800\n",
      "Loss:0.8411443065358679\n",
      "Pred:[1 0 0 1 1 1 0 1]\n",
      "True:[0 1 1 1 1 1 0 1]\n",
      "76 + 49 = 157\n",
      "------------\n",
      "iters:2900\n",
      "Loss:0.13184878694650037\n",
      "Pred:[0 0 1 0 1 1 0 1]\n",
      "True:[0 0 1 0 1 1 0 1]\n",
      "34 + 11 = 45\n",
      "------------\n",
      "iters:3000\n",
      "Loss:0.10656735037188261\n",
      "Pred:[1 0 1 0 0 1 1 0]\n",
      "True:[1 0 1 0 0 1 1 0]\n",
      "109 + 57 = 166\n",
      "------------\n",
      "iters:3100\n",
      "Loss:0.07934093285924683\n",
      "Pred:[1 0 1 0 1 0 0 1]\n",
      "True:[1 0 1 0 1 0 0 1]\n",
      "57 + 112 = 169\n",
      "------------\n",
      "iters:3200\n",
      "Loss:0.16876850518907877\n",
      "Pred:[1 1 0 1 1 0 0 0]\n",
      "True:[1 1 0 1 1 0 0 0]\n",
      "93 + 123 = 216\n",
      "------------\n",
      "iters:3300\n",
      "Loss:0.05624485978434935\n",
      "Pred:[0 1 0 1 0 1 0 1]\n",
      "True:[0 1 0 1 0 1 0 1]\n",
      "35 + 50 = 85\n",
      "------------\n",
      "iters:3400\n",
      "Loss:0.10829253528340128\n",
      "Pred:[0 1 0 0 0 0 1 0]\n",
      "True:[0 1 0 0 0 0 1 0]\n",
      "27 + 39 = 66\n",
      "------------\n",
      "iters:3500\n",
      "Loss:0.07116247405232998\n",
      "Pred:[1 1 0 0 0 1 0 1]\n",
      "True:[1 1 0 0 0 1 0 1]\n",
      "71 + 126 = 197\n",
      "------------\n",
      "iters:3600\n",
      "Loss:0.04514159082567855\n",
      "Pred:[0 1 1 1 1 1 0 0]\n",
      "True:[0 1 1 1 1 1 0 0]\n",
      "43 + 81 = 124\n",
      "------------\n",
      "iters:3700\n",
      "Loss:0.03289339469738701\n",
      "Pred:[0 1 0 1 1 0 0 1]\n",
      "True:[0 1 0 1 1 0 0 1]\n",
      "44 + 45 = 89\n",
      "------------\n",
      "iters:3800\n",
      "Loss:0.27252441962794655\n",
      "Pred:[1 0 0 0 1 1 0 1]\n",
      "True:[1 0 0 0 1 1 0 1]\n",
      "30 + 111 = 141\n",
      "------------\n",
      "iters:3900\n",
      "Loss:0.014480742618614945\n",
      "Pred:[0 1 1 1 1 0 0 0]\n",
      "True:[0 1 1 1 1 0 0 0]\n",
      "76 + 44 = 120\n",
      "------------\n",
      "iters:4000\n",
      "Loss:0.010651969008371745\n",
      "Pred:[0 0 0 1 0 0 1 0]\n",
      "True:[0 0 0 1 0 0 1 0]\n",
      "7 + 11 = 18\n",
      "------------\n",
      "iters:4100\n",
      "Loss:0.015293325037389005\n",
      "Pred:[0 0 1 1 1 0 0 0]\n",
      "True:[0 0 1 1 1 0 0 0]\n",
      "38 + 18 = 56\n",
      "------------\n",
      "iters:4200\n",
      "Loss:0.014919982558902074\n",
      "Pred:[1 1 0 1 0 0 0 1]\n",
      "True:[1 1 0 1 0 0 0 1]\n",
      "102 + 107 = 209\n",
      "------------\n",
      "iters:4300\n",
      "Loss:0.01637803128734032\n",
      "Pred:[1 0 0 1 1 0 1 1]\n",
      "True:[1 0 0 1 1 0 1 1]\n",
      "63 + 92 = 155\n",
      "------------\n",
      "iters:4400\n",
      "Loss:0.006814296718207121\n",
      "Pred:[0 0 0 0 1 1 0 0]\n",
      "True:[0 0 0 0 1 1 0 0]\n",
      "6 + 6 = 12\n",
      "------------\n",
      "iters:4500\n",
      "Loss:0.0072220543876404236\n",
      "Pred:[0 0 1 1 1 1 1 0]\n",
      "True:[0 0 1 1 1 1 1 0]\n",
      "4 + 58 = 62\n",
      "------------\n",
      "iters:4600\n",
      "Loss:0.011619813246795695\n",
      "Pred:[0 1 0 1 1 1 1 1]\n",
      "True:[0 1 0 1 1 1 1 1]\n",
      "35 + 60 = 95\n",
      "------------\n",
      "iters:4700\n",
      "Loss:0.009782146898018195\n",
      "Pred:[1 0 1 1 1 0 0 0]\n",
      "True:[1 0 1 1 1 0 0 0]\n",
      "98 + 86 = 184\n",
      "------------\n",
      "iters:4800\n",
      "Loss:0.008841386887366041\n",
      "Pred:[0 0 0 1 1 0 1 1]\n",
      "True:[0 0 0 1 1 0 1 1]\n",
      "7 + 20 = 27\n",
      "------------\n",
      "iters:4900\n",
      "Loss:0.00497373905119289\n",
      "Pred:[1 0 1 1 1 1 0 1]\n",
      "True:[1 0 1 1 1 1 0 1]\n",
      "108 + 81 = 189\n",
      "------------\n",
      "iters:5000\n",
      "Loss:0.005520647779263517\n",
      "Pred:[1 0 0 1 0 0 0 1]\n",
      "True:[1 0 0 1 0 0 0 1]\n",
      "50 + 95 = 145\n",
      "------------\n",
      "iters:5100\n",
      "Loss:0.006175488219609031\n",
      "Pred:[1 1 0 0 0 1 0 1]\n",
      "True:[1 1 0 0 0 1 0 1]\n",
      "107 + 90 = 197\n",
      "------------\n",
      "iters:5200\n",
      "Loss:0.005607167081286331\n",
      "Pred:[1 1 1 0 0 0 0 1]\n",
      "True:[1 1 1 0 0 0 0 1]\n",
      "98 + 127 = 225\n",
      "------------\n",
      "iters:5300\n",
      "Loss:0.004143433555554139\n",
      "Pred:[0 0 1 1 1 1 0 1]\n",
      "True:[0 0 1 1 1 1 0 1]\n",
      "25 + 36 = 61\n",
      "------------\n",
      "iters:5400\n",
      "Loss:0.005412536658482966\n",
      "Pred:[0 1 1 0 0 0 1 1]\n",
      "True:[0 1 1 0 0 0 1 1]\n",
      "47 + 52 = 99\n",
      "------------\n",
      "iters:5500\n",
      "Loss:0.0024140661980705026\n",
      "Pred:[0 1 1 0 0 0 0 0]\n",
      "True:[0 1 1 0 0 0 0 0]\n",
      "37 + 59 = 96\n",
      "------------\n",
      "iters:5600\n",
      "Loss:0.004833620123800285\n",
      "Pred:[0 0 1 1 0 1 1 1]\n",
      "True:[0 0 1 1 0 1 1 1]\n",
      "10 + 45 = 55\n",
      "------------\n",
      "iters:5700\n",
      "Loss:0.0012440815036296367\n",
      "Pred:[1 0 0 0 0 1 1 0]\n",
      "True:[1 0 0 0 0 1 1 0]\n",
      "37 + 97 = 134\n",
      "------------\n",
      "iters:5800\n",
      "Loss:0.0029714293451742523\n",
      "Pred:[0 1 0 0 1 0 0 1]\n",
      "True:[0 1 0 0 1 0 0 1]\n",
      "73 + 0 = 73\n",
      "------------\n",
      "iters:5900\n",
      "Loss:0.004689017320337064\n",
      "Pred:[0 1 0 0 0 0 0 0]\n",
      "True:[0 1 0 0 0 0 0 0]\n",
      "26 + 38 = 64\n",
      "------------\n",
      "iters:6000\n",
      "Loss:0.0008630041709096442\n",
      "Pred:[0 0 1 1 0 0 1 0]\n",
      "True:[0 0 1 1 0 0 1 0]\n",
      "9 + 41 = 50\n",
      "------------\n",
      "iters:6100\n",
      "Loss:0.0029584060807972003\n",
      "Pred:[0 1 0 0 1 0 1 1]\n",
      "True:[0 1 0 0 1 0 1 1]\n",
      "70 + 5 = 75\n",
      "------------\n",
      "iters:6200\n",
      "Loss:0.003975252174599373\n",
      "Pred:[1 0 1 0 1 0 0 0]\n",
      "True:[1 0 1 0 1 0 0 0]\n",
      "50 + 118 = 168\n",
      "------------\n",
      "iters:6300\n",
      "Loss:0.003579144761116939\n",
      "Pred:[1 0 1 1 0 1 1 1]\n",
      "True:[1 0 1 1 0 1 1 1]\n",
      "81 + 102 = 183\n",
      "------------\n",
      "iters:6400\n",
      "Loss:0.003032171993080525\n",
      "Pred:[1 0 0 1 0 0 0 0]\n",
      "True:[1 0 0 1 0 0 0 0]\n",
      "52 + 92 = 144\n",
      "------------\n",
      "iters:6500\n",
      "Loss:0.002357339923444445\n",
      "Pred:[0 1 1 1 0 0 1 1]\n",
      "True:[0 1 1 1 0 0 1 1]\n",
      "82 + 33 = 115\n",
      "------------\n",
      "iters:6600\n",
      "Loss:0.0011634411324606315\n",
      "Pred:[1 0 1 1 1 0 1 0]\n",
      "True:[1 0 1 1 1 0 1 0]\n",
      "79 + 107 = 186\n",
      "------------\n",
      "iters:6700\n",
      "Loss:0.0008029245524473867\n",
      "Pred:[0 1 0 0 1 1 1 0]\n",
      "True:[0 1 0 0 1 1 1 0]\n",
      "17 + 61 = 78\n",
      "------------\n",
      "iters:6800\n",
      "Loss:0.002780258957921167\n",
      "Pred:[0 1 1 0 1 0 0 0]\n",
      "True:[0 1 1 0 1 0 0 0]\n",
      "34 + 70 = 104\n",
      "------------\n",
      "iters:6900\n",
      "Loss:0.0023746476984649826\n",
      "Pred:[0 1 0 0 1 1 1 0]\n",
      "True:[0 1 0 0 1 1 1 0]\n",
      "4 + 74 = 78\n",
      "------------\n",
      "iters:7000\n",
      "Loss:0.002635370229904707\n",
      "Pred:[1 0 0 1 0 0 1 0]\n",
      "True:[1 0 0 1 0 0 1 0]\n",
      "48 + 98 = 146\n",
      "------------\n",
      "iters:7100\n",
      "Loss:0.0023032574146336863\n",
      "Pred:[0 1 1 0 0 0 0 1]\n",
      "True:[0 1 1 0 0 0 0 1]\n",
      "45 + 52 = 97\n",
      "------------\n",
      "iters:7200\n",
      "Loss:0.0006713124389886513\n",
      "Pred:[1 0 0 1 1 0 0 0]\n",
      "True:[1 0 0 1 1 0 0 0]\n",
      "39 + 113 = 152\n",
      "------------\n",
      "iters:7300\n",
      "Loss:0.0006309674835491653\n",
      "Pred:[0 0 1 1 0 0 1 0]\n",
      "True:[0 0 1 1 0 0 1 0]\n",
      "39 + 11 = 50\n",
      "------------\n",
      "iters:7400\n",
      "Loss:0.0015922428280683602\n",
      "Pred:[0 1 1 0 0 1 1 1]\n",
      "True:[0 1 1 0 0 1 1 1]\n",
      "6 + 97 = 103\n",
      "------------\n",
      "iters:7500\n",
      "Loss:0.0024870801237977863\n",
      "Pred:[0 1 1 1 1 1 1 1]\n",
      "True:[0 1 1 1 1 1 1 1]\n",
      "45 + 82 = 127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "iters:7600\n",
      "Loss:0.0023816357986003794\n",
      "Pred:[1 0 1 1 1 1 1 0]\n",
      "True:[1 0 1 1 1 1 1 0]\n",
      "106 + 84 = 190\n",
      "------------\n",
      "iters:7700\n",
      "Loss:0.0019451505057010458\n",
      "Pred:[1 0 1 0 1 1 1 1]\n",
      "True:[1 0 1 0 1 1 1 1]\n",
      "105 + 70 = 175\n",
      "------------\n",
      "iters:7800\n",
      "Loss:0.0009882165073894637\n",
      "Pred:[1 1 0 1 1 0 0 0]\n",
      "True:[1 1 0 1 1 0 0 0]\n",
      "95 + 121 = 216\n",
      "------------\n",
      "iters:7900\n",
      "Loss:0.0015429192681856493\n",
      "Pred:[1 1 0 0 0 0 1 1]\n",
      "True:[1 1 0 0 0 0 1 1]\n",
      "78 + 117 = 195\n",
      "------------\n",
      "iters:8000\n",
      "Loss:0.0005543140910712863\n",
      "Pred:[1 0 0 1 1 0 1 0]\n",
      "True:[1 0 0 1 1 0 1 0]\n",
      "107 + 47 = 154\n",
      "------------\n",
      "iters:8100\n",
      "Loss:0.0014387731699986258\n",
      "Pred:[0 1 1 0 0 1 0 1]\n",
      "True:[0 1 1 0 0 1 0 1]\n",
      "79 + 22 = 101\n",
      "------------\n",
      "iters:8200\n",
      "Loss:0.001769254547193303\n",
      "Pred:[0 0 0 1 0 1 0 0]\n",
      "True:[0 0 0 1 0 1 0 0]\n",
      "12 + 8 = 20\n",
      "------------\n",
      "iters:8300\n",
      "Loss:0.0017749199814349195\n",
      "Pred:[0 0 1 1 1 0 0 0]\n",
      "True:[0 0 1 1 1 0 0 0]\n",
      "34 + 22 = 56\n",
      "------------\n",
      "iters:8400\n",
      "Loss:0.0006126589699232333\n",
      "Pred:[0 1 1 0 0 0 1 0]\n",
      "True:[0 1 1 0 0 0 1 0]\n",
      "7 + 91 = 98\n",
      "------------\n",
      "iters:8500\n",
      "Loss:0.0013848768759580764\n",
      "Pred:[1 0 0 0 1 1 0 1]\n",
      "True:[1 0 0 0 1 1 0 1]\n",
      "53 + 88 = 141\n",
      "------------\n",
      "iters:8600\n",
      "Loss:0.0014411835877657009\n",
      "Pred:[0 0 1 0 1 0 1 1]\n",
      "True:[0 0 1 0 1 0 1 1]\n",
      "23 + 20 = 43\n",
      "------------\n",
      "iters:8700\n",
      "Loss:0.00035539134440022973\n",
      "Pred:[1 0 1 0 1 1 0 0]\n",
      "True:[1 0 1 0 1 1 0 0]\n",
      "53 + 119 = 172\n",
      "------------\n",
      "iters:8800\n",
      "Loss:0.001353005359924085\n",
      "Pred:[0 0 1 1 1 0 0 0]\n",
      "True:[0 0 1 1 1 0 0 0]\n",
      "16 + 40 = 56\n",
      "------------\n",
      "iters:8900\n",
      "Loss:0.0014598699813507838\n",
      "Pred:[0 1 1 0 0 1 0 0]\n",
      "True:[0 1 1 0 0 1 0 0]\n",
      "86 + 14 = 100\n",
      "------------\n",
      "iters:9000\n",
      "Loss:0.0007623233958032332\n",
      "Pred:[0 1 1 0 0 0 0 1]\n",
      "True:[0 1 1 0 0 0 0 1]\n",
      "88 + 9 = 97\n",
      "------------\n",
      "iters:9100\n",
      "Loss:0.0012159202234040267\n",
      "Pred:[1 0 1 1 0 1 0 1]\n",
      "True:[1 0 1 1 0 1 0 1]\n",
      "89 + 92 = 181\n",
      "------------\n",
      "iters:9200\n",
      "Loss:0.0011482071786280816\n",
      "Pred:[1 0 0 1 1 0 0 1]\n",
      "True:[1 0 0 1 1 0 0 1]\n",
      "69 + 84 = 153\n",
      "------------\n",
      "iters:9300\n",
      "Loss:0.0013096922302229717\n",
      "Pred:[0 1 1 1 1 0 1 0]\n",
      "True:[0 1 1 1 1 0 1 0]\n",
      "20 + 102 = 122\n",
      "------------\n",
      "iters:9400\n",
      "Loss:0.001330567191249565\n",
      "Pred:[1 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 0 0 0]\n",
      "120 + 8 = 128\n",
      "------------\n",
      "iters:9500\n",
      "Loss:0.000340046750912289\n",
      "Pred:[0 1 1 0 1 1 1 0]\n",
      "True:[0 1 1 0 1 1 1 0]\n",
      "29 + 81 = 110\n",
      "------------\n",
      "iters:9600\n",
      "Loss:0.00034545566908197656\n",
      "Pred:[1 0 1 0 1 1 0 0]\n",
      "True:[1 0 1 0 1 1 0 0]\n",
      "89 + 83 = 172\n",
      "------------\n",
      "iters:9700\n",
      "Loss:0.0008865697923943426\n",
      "Pred:[1 1 1 0 0 1 0 1]\n",
      "True:[1 1 1 0 0 1 0 1]\n",
      "122 + 107 = 229\n",
      "------------\n",
      "iters:9800\n",
      "Loss:0.0011453316417142508\n",
      "Pred:[1 1 0 0 0 1 1 1]\n",
      "True:[1 1 0 0 0 1 1 1]\n",
      "91 + 108 = 199\n",
      "------------\n",
      "iters:9900\n",
      "Loss:0.0009746724282374789\n",
      "Pred:[0 0 1 0 0 1 1 0]\n",
      "True:[0 0 1 0 0 1 1 0]\n",
      "6 + 32 = 38\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(iters_num):\n",
    "    # A, B初期化 (a + b = d)\n",
    "    a_int = np.random.randint(largest_number / 2)\n",
    "    a_bin = binary[a_int] # binary encoding\n",
    "    b_int = np.random.randint(largest_number / 2)\n",
    "    b_bin = binary[b_int] # binary encoding\n",
    "    \n",
    "    # 正解データ\n",
    "    d_int = a_int + b_int\n",
    "    d_bin = binary[d_int]\n",
    "    \n",
    "    # 出力バイナリ\n",
    "    out_bin = np.zeros_like(d_bin)\n",
    "    \n",
    "    # 時系列全体の誤差\n",
    "    all_loss = 0    \n",
    "    \n",
    "    # 時系列ループ\n",
    "    for t in range(binary_dim):\n",
    "        # 入力値\n",
    "        X = np.array([a_bin[-t - 1], b_bin[-t - 1]]).reshape(1, -1)\n",
    "        # 時刻tにおける正解データ\n",
    "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
    "        \n",
    "        u[:, t+1] = np.dot(X, W_in) + np.dot(z[:, t].reshape(1, -1), W)\n",
    "        z[:, t+1] = functions.sigmoid(u[:, t+1])\n",
    "\n",
    "        y[:, t] = functions.sigmoid(np.dot(z[:, t+1].reshape(1, -1), W_out))\n",
    "\n",
    "        # 誤差関数はMSEを使用する\n",
    "        loss = functions.mean_squared_error(dd, y[:,t])\n",
    "        \n",
    "        delta_out[:, t] = functions.d_mean_squared_error(dd, y[:, t]) * functions.d_sigmoid(y[:, t])        \n",
    "        \n",
    "        all_loss += loss\n",
    "\n",
    "        out_bin[binary_dim - t - 1] = np.round(y[:, t])\n",
    "    \n",
    "    for t in range(binary_dim)[::-1]:\n",
    "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
    "\n",
    "        delta[:,t] = (np.dot(delta[:, t+1].T, W.T) + np.dot(delta_out[:, t].T, W_out.T)) * functions.d_sigmoid(u[:, t+1])\n",
    "\n",
    "        # 勾配の更新\n",
    "        W_out_grad += np.dot(z[:, t+1].reshape(-1,1), delta_out[:, t].reshape(-1, 1))\n",
    "        W_grad += np.dot(z[:, t].reshape(-1, 1), delta[:, t].reshape(1, -1))\n",
    "        W_in_grad += np.dot(X.T, delta[:, t].reshape(1, -1))\n",
    "    \n",
    "    # 更新された勾配を適用する\n",
    "    W_in -= learning_rate * W_in_grad\n",
    "    W_out -= learning_rate * W_out_grad\n",
    "    W -= learning_rate * W_grad\n",
    "    \n",
    "    # 更新用の勾配を初期化\n",
    "    W_in_grad *= 0\n",
    "    W_out_grad *= 0\n",
    "    W_grad *= 0\n",
    "    \n",
    "    # 途中経過の出力\n",
    "    if i % plot_interval == 0:\n",
    "        all_losses.append(all_loss)\n",
    "        \n",
    "        print(f\"iters: {i}\")\n",
    "        print(f\"Loss: {all_loss}\")\n",
    "        print(f\"Pred: {out_bin}\")\n",
    "        print(f\"True: {d_bin}\")\n",
    "        \n",
    "        out_int = 0\n",
    "        \n",
    "        for index, x in enumerate(reversed(out_bin)):\n",
    "            out_int += x * pow(2, index)\n",
    "        \n",
    "        print(f\"{a_int} + {b_int} = {out_int}\")\n",
    "        print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グラフの描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lists = range(0, iters_num, plot_interval)\n",
    "plt.plot(lists, all_losses, label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 考察\n",
    "iters:1600あたりで予測値が正解と一致するようになり、iters:2100あたりから正解値とよく一致するようになった。 <br/>\n",
    "iters:3000あたりからは、かなりの精度で正解と一致するようになった。 <br />\n",
    "lossのグラフを見ても、iters:3000あたりからは0.1未満になるようになった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
